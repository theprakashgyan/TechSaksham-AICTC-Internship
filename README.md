# TechSaksham-AICTC-Internship

### Project: AICTE Internship on AI: Transformative Learning with TechSaksham – A joint CSR initiative of Microsoft & SAP

### Objective

To explore and implement an image generation system using Stable Diffusion and ComfyUI, enabling the creation of high-quality and diverse images from textual prompts or noise initialization, while providing an interactive and user-friendly interface for creative AI applications.

### Key Activities

-**Literature Review:**
Studied diffusion models, U-Net architecture, and generative AI techniques.

-**Environment Setup:**
Configured hardware and installed required software, libraries, and tools for Stable Diffusion and ComfyUI.

-**Model Integration:**
Loaded and fine-tuned the Stable Diffusion model using the Diffusers library.
Integrated the model with ComfyUI for interactive visualization.

-**Workflow Development:**
Designed and implemented a pipeline for generating images using text prompts and noise initialization.

-**Parameter Tuning:**
Experimented with resolution, guidance scales, and inference steps for optimal image quality.

-**Image Generation and Analysis:**
Generated diverse images, capturing various creative and realistic elements.
Evaluated image quality and performance metrics.

-**Snapshot Documentation:**
Captured and analyzed generated image snapshots for report documentation.

### Understanding Connection 

**1. Load Checkpoint**
Loads the pre-trained Stable Diffusion model weights (.ckpt files).
Provides essential parameters for image generation and allows model version switching.

**2. CLIP Text Encoder**
Converts the user’s text prompt into a high-dimensional vector representation.
Guides the diffusion model to generate images aligned with the prompt.

**3. KSampler**
Handles the iterative denoising process, transforming random noise into meaningful images.
Key components:
*Noise Schedule:* Gradual noise removal.
*Sampling Method:* Techniques like Euler or DDIM for quality control.
*Guidance Scale:* Defines how strictly the output follows the prompt.

**4. VAE Decode (Variational Autoencoder Decode)**
Converts the compressed latent data back into a viewable image.
Ensures that fine image details are reconstructed accurately.

### Snap Shots of Result:

**Snapshot 1:* :
![Screenshot 2025-02-11 212440](https://github.com/user-attachments/assets/bdbcc8c8-8532-4194-9556-5bd754fe36ba)
![ComfyUI_00013_](https://github.com/user-attachments/assets/e78d7838-28f7-4f15-bf77-3ccfdc466a9a)

**Snapshot 2:* :
![Screenshot 2025-02-11 211419](https://github.com/user-attachments/assets/a0ed18ea-069b-4a9f-b2e2-5edb672bf841)
![ComfyUI_00012_](https://github.com/user-attachments/assets/c309713e-0e8c-4805-ba44-a7c57c3f5bc3)

**Snapshot 3:* :
![Screenshot 2025-02-11 203005](https://github.com/user-attachments/assets/5ff72e32-d1ec-4f2d-8af4-1fe9318c5e49)
![ComfyUI_00006_](https://github.com/user-attachments/assets/9fe06c92-9c2a-4c4c-acfa-3ab018b8b490)



